behaviors:
  PerformanceAgent:
    trainer_type: ppo

    hyperparameters:
      batch_size: 1024           # larger batch because we aggregate from 8 envs
      buffer_size: 16384         # (100 steps/env) * (8 envs) * ~20 episodes/iteration
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.15
      lambd: 0.95                # smaller for simulated environment
      num_epoch: 3
      learning_rate_schedule: linear

    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 2
      vis_encode_type: simple

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0

    keep_checkpoints: 10
    max_steps: 1000000

    # Simulated FPS runs very fast â†’ shorter trajectories help PPO
    time_horizon: 64

    # More frequent stats
    summary_freq: 2000
